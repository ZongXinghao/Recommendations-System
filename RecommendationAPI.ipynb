{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommender system with Spark machine learning\n",
    "\n",
    "In this Jupyter notebook, you will use Apache Spark and the Spark machine learning library to build a recommender system for movies with a data set from MovieLens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens\n",
    "\n",
    "MovieLens is a project developed by GroupLens, a research laboratory at the University of Minnesota. MovieLens provides an online movie recommender application that uses anonymously-collected data to improve recommender algorithms. Anyone can try the app for free and get movies recommendations. To help people develop the best recommendation algorithms, MovieLens also released several data sets. In this notebook, you'll use the latest data set, which has two sizes.\n",
    "\n",
    "The full data set consists of more than 24 million ratings across more than 40,000 movies by more than 250,000 users. The file size is kept under 1GB by using indexes instead of full string names.\n",
    "\n",
    "The small data set that you'll use in this notebook is a subset of the full data set. It's generally a good idea to start building a working program with a small data set to get faster performance while interacting, exploring, and getting errors with your data. When you have a fully working program, you can apply the same code to the larger data set, possibly on a larger cluster of processors. You can also minimize memory consumption by limiting the data volume as much as possible, for example, by using indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark machine learning library\n",
    "Apache Sparkâ€™s machine learning library makes practical machine learning scalable and easy. The library consists of common machine learning algorithms and utilities, including classification, regression, clustering, collaborative filtering (this notebook!), dimensionality reduction, lower-level optimization primitives, and higher-level pipeline APIs.\n",
    "\n",
    "The library has two packages:\n",
    "\n",
    "spark.mllib contains the original API that handles data in RDDs. It's in maintenance mode, but fully supported.\n",
    "spark.ml contains a newer API for constructing ML pipelines. It handles data in DataFrames. It's being actively enhanced.\n",
    "You'll use the spark.ml package in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* 1 Load the data   \n",
    "* 2 Explore the data with Spark APIs\n",
    "* 3 Build the recommender system\n",
    "* 4 Save results\n",
    "* 5 Distribute results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/hatv/dsx_SparkLessons/blob/master/machine-learning.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Movie Recommender').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"Data/movies.csv\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"Data/ratings.csv\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the data with Spark APIs\n",
    "\n",
    "You'll use the Spark DataFrame API and SparkSQL to look at the data. The Spark DataFrame API and SparkSQL are high level APIs to query and transform Spark DataFrames. See DataFrame documentation for a detailed description of the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.printSchema()\n",
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the describe() method to see the count, mean, standard deviation, minimum, and maximum values for the data in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all of these statistics are actually meaningful!\n",
    "\n",
    "You can use specific methods from the DataFrame API to compute any statistic:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of different users: \" + str(ratings.select('userId').distinct().count()))\n",
    "print(\"Number of different movies: \" + str(ratings.select('movieId').distinct().count()))\n",
    "print(\"Number of movies with at least one rating strictly higher than 4: \" + str(ratings.filter('rating > 4').select('movieId').distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also leverage your SQL knowledge to query the data. Spark version 2.0 is ANSI SQL-92 compliant and can run the 99 TPC-DS queries.\n",
    "\n",
    "Find the number of movies with ratings higher than 4 again, this time with SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.createOrReplaceTempView('ratings')\n",
    "spark.sql(\"SELECT COUNT(DISTINCT(movieId)) AS nb FROM ratings WHERE rating > 4\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily switch between Spark distributed DataFrames and pandas local DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the recommender system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different methods for building a recommender system, such as, user-based, content-based, or collaborative filtering. Collaborative filtering calculates recommendations based on similarities between users and products. For example, collaborative filtering assumes that users who give the similar ratings on the same movies will also have similar opinions on movies that they haven't seen.\n",
    "\n",
    "The alternating least squares (ALS) algorithm provides collaborative filtering between users and products to find products that the customers might like, based on their previous ratings.\n",
    "\n",
    "In this case, the ALS algorithm will create a matrix of all users versus all movies. Most cells in the matrix will be empty. An empty cell means the user hasn't reviewed the movie yet. The ALS algorithm will fill in the probable (predicted) ratings, based on similarities between user ratings. The algorithm uses the least squares computation to minimize the estimation errors, and alternates between solving for movie factors and solving for user factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the ratings matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *, (100 - (100 * nb_ratings/matrix_size)) AS sparsity\n",
    "    FROM (\n",
    "        SELECT nb_users, nb_movies, nb_ratings, nb_users * nb_movies AS matrix_size\n",
    "        FROM (\n",
    "            SELECT COUNT(*) AS nb_ratings, COUNT(DISTINCT(movieId)) AS nb_movies, COUNT(DISTINCT(userId)) AS nb_users\n",
    "            FROM ratings\n",
    "        )\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Less than 2% of the matrix is filled!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Split the data set\n",
    "Split your ratings data set between an 80% training data set and a 20% test data set. Then rerun the steps to train the model on the training set, run it on the test set, and evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingRatings, testRatings = ratings.randomSplit([80.0, 20.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
    "model = als.fit(trainingRatings)\n",
    "predictions = model.transform(testRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgRatings = ratings.select('rating').groupBy().avg().first()[0]\n",
    "print(\"The average rating in the dataset is: \" + str(avgRatings))\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "print(\"The root mean squared error (RMSE) for our model is: \" + str(evaluator.evaluate(predictions.na.fill(avgRatings))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, you get lower performance than with the previous model, but you're protected against overfitting: you will actually get this level of performance on new incoming data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Recommend movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recommend movies for a specific user, create a function that applies the trained model, ALSModel, on the list of movies that the user hasn't yet rated.\n",
    "\n",
    "Create a recommendMovies function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def recommendMovies(model, user, nbRecommendations):\n",
    "    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame\n",
    "    dataSet = ratings.select(\"movieId\").distinct().withColumn(\"userId\", lit(user))\n",
    "\n",
    "    # Create a Spark DataFrame with the movies that have already been rated by this user\n",
    "    moviesAlreadyRated = ratings.filter(ratings.userId == user).select(\"movieId\", \"userId\")\n",
    "\n",
    "    # Apply the recommender system to the data set without the already rated movies to predict ratings\n",
    "    predictions = model.transform(dataSet.subtract(moviesAlreadyRated)).dropna().orderBy(\"prediction\", ascending=False).limit(nbRecommendations).select(\"movieId\", \"prediction\")\n",
    "\n",
    "    # Join with the movies DataFrame to get the movies titles and genres\n",
    "    recommendations = predictions.join(movies, predictions.movieId == movies.movieId).select(predictions.movieId, movies.title, movies.genres, predictions.prediction)\n",
    "\n",
    "    recommendations.show(truncate=False)\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run this function to recommend 10 movies for three different users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recommendations for user 133:\")\n",
    "recommendMovies(model, 133, 10)\n",
    "print(\"Recommendations for user 471:\")\n",
    "recommendMovies(model, 471, 10)\n",
    "print(\"Recommendations for user 496:\")\n",
    "recommendMovies(model, 496, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Save Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When using the model later in realtime, we need to save the results. We can score the model in realtime in two ways:\n",
    "    1. Save the model and score it in realtime\n",
    "    2. Save the table with predictions, so you can lookup the user (item) in realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.write.mode(\"overwrite\").save(\"Data/predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.toPandas().to_csv(\"Data/predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distribute Results\n",
    "\n",
    "Next we build a flask API that has two functions or endpoints:\n",
    "- Giving top recommendations for a user: /ratings/top. By calling (POST) this endpoint with a userId en optionally a count in the body, the top recommended items together with the prediction scores are returned.\n",
    "- Compute the score of a user-item pair: /ratings/calculateScore. By calling (POST) this endpoint with a list of userId and movieId in the body, the prediction for this user-item pair is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize app\n",
    "app = Flask(__name__)\n",
    "\n",
    "## Endpoint 1: Top ratings for a user\n",
    "@app.route(\"/ratings/top\", methods=[\"GET\"])\n",
    "def top_ratings():\n",
    "\n",
    "    ## read the parameters of the API call\n",
    "    userId_str = request.args.get(\"userId\")\n",
    "    try:\n",
    "        userId = int(userId_str)\n",
    "    except:\n",
    "        return \"'userId' is required and should be an Integer.\"\n",
    "        sys.exit(\"'userId' is required and should be an Integer.\")\n",
    "        \n",
    "    try:\n",
    "        count = int(count_str)\n",
    "    except:\n",
    "        count = 5\n",
    "    \n",
    "    # Recommend\n",
    "    recommendations = recommendMovies(model, userId, count)\n",
    "    \n",
    "    # Transform to Python dictionary\n",
    "    recommendations.collect()\n",
    "    top_ratings = list(map(lambda a: a.asDict(), recommendations.collect()))\n",
    "    \n",
    "    # Return the result to the API\n",
    "    return jsonify(top_ratings)\n",
    "\n",
    "## Endpoint 2: calculate scores for a user-movie combination\n",
    "@app.route(\"/ratings/calculateScore\", methods=[\"GET\"])\n",
    "def newScore():\n",
    "    ## read the parameters of the API call\n",
    "    userId_str = request.args.get(\"userId\")\n",
    "    try:\n",
    "        userId = int(userId_str)\n",
    "    except:\n",
    "        return \"'userId' is required and should be an Integer.\"\n",
    "        sys.exit(\"'userId' is required and should be an Integer.\")\n",
    "        \n",
    "    ## read the parameters of the API call\n",
    "    movieId_str = request.args.get(\"movieId\")\n",
    "    try:\n",
    "        movieId = int(movieId_str)\n",
    "    except:\n",
    "        return \"'userId' is required and should be an Integer.\"\n",
    "        sys.exit(\"'userId' is required and should be an Integer.\")\n",
    "    \n",
    "    # Create a Spark dataframe based in the scores list\n",
    "    ratings = spark.createDataFrame(content) \n",
    "\n",
    "    # Predict estimated ratings for the user\n",
    "    res = model.transform(ratings)\n",
    "    \n",
    "    # Collect results in a list\n",
    "    newPredictions = list(res.toPandas().T.to_dict().values())\n",
    "    \n",
    "    # Return the result to the API\n",
    "    return jsonify(newPredictions) \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='localhost', port=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
